{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde20e32-4d70-41ad-865e-daec3d24e4bb",
   "metadata": {},
   "source": [
    "# HF Unit 1 - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd0be9ae-36e7-4fb6-8b44-263c9f90439d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998713731765747},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996659755706787},\n",
       " {'label': 'NEGATIVE', 'score': 0.8500027060508728}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"This Hugging face course is the best ever!\",\n",
    "    \"\"\"NVIDIA's quarterly earnings disappointed its shareholders,\n",
    "    with the market opening 50 basis points lower than the previous day's close\"\"\",\n",
    "    \"\"\"NVIDIA's quarterly earnings was met with a lukewarm response,\n",
    "    with the market opening 5 basis points higher than the previous day's close, producing a 5% YTD return for investors.\"\"\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6843318e-d6c5-447e-be92-18209a36a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8647aef-7fe6-4c00-a888-0fd8a3cd8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(levelname)s %(module)s %(funcName)s %(message)s',\n",
    "    handlers=[\n",
    "        stream_handler,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fec6098-683b-4434-9b3b-5ff1bbb44b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 20:44:57,882 INFO instantiator <module> Created a temporary directory at /tmp/tmptdp92n5t\n",
      "2023-09-01 20:44:57,883 INFO instantiator _write Writing /tmp/tmptdp92n5t/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM,  LlamaTokenizer, BertTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "#model_id = \"distilbert-base-uncased\"\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "#model_id = \"codellama/CodeLlama-7b-hf\"\n",
    "# model_id = \"bert-base-uncased\"\n",
    "#model_id = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "#model = AutoModelForCausalLM.from_pretrained(model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a792d9a8-7ce7-4325-9d47-3ec743ad8be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 20:53:06,768 INFO 2476881417 <module> Running Tokenizer ...\n",
      "2023-09-01 20:53:06,770 DEBUG connectionpool _get_conn Resetting dropped connection: huggingface.co\n",
      "2023-09-01 20:53:07,032 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2023-09-01 20:53:07,355 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-09-01 20:53:07,605 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/configuration_RW.py HTTP/1.1\" 200 0\n",
      "2023-09-01 20:53:07,866 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-09-01 20:53:08,118 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/modelling_RW.py HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb34b72411df4957abd6a268dbde7a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 20:53:12,372 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2023-09-01 20:53:12,414 INFO 2476881417 <module> Making new pipeline w/GPU ...\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "with torch.device(\"cuda\"):\n",
    "    logging.info(\"Running Tokenizer ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        #load_in_4bit=True,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    #model = BetterTransformer.transform(model)\n",
    "    logging.info(\"Making new pipeline w/GPU ...\")\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_4bit,\n",
    "        use_cache=True,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        max_length=500,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        # device=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55955a04-e75a-4544-a44e-5febf3c30a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 20:55:01,882 INFO 1916992056 <module> Generating sequences\n",
      "2023-09-01 20:55:10,013 INFO 1916992056 <module> Printing sequences\n",
      "2023-09-01 20:55:10,014 INFO 1916992056 <module> Result: Sally is a kindergarden teacher and Val is her student.Sally needs to contact Val's parent Soph to discuss her child's performance in school.Val is always late for class, and Sally needs to highlight to Soph about it.Val is extroverted and has made many new friends in school.\n",
      "       Soph: Morning Sally! How is Val doing in school?\n",
      "       Sally: Hi Soph, Val is doing very well in all her classes. However, she is always late for school. We are concerned that she may be having trouble waking up in the morning.\n",
      "       Soph: I see. Well, I'm sure there's nothing to be concerned about. But if you notice any change in her routine or performance, please let me know so we can address it.\n",
      "2023-09-01 20:55:10,014 INFO 1916992056 <module> Operation Complete\n"
     ]
    }
   ],
   "source": [
    "with torch.device(\"cuda\"):\n",
    "    logging.info(\"Generating sequences\")\n",
    "    sequences = pipeline(\n",
    "       \"\"\"Sally is a kindergarden teacher and Val is her student.Sally needs to contact Val's parent Soph to discuss her child's performance in school.Val is always late for class, and Sally needs to highlight to Soph about it.Val is extroverted and has made many new friends in school.\n",
    "       Soph: Morning Sally! How is Val doing in school?\n",
    "       Sally:\"\"\",\n",
    "    )\n",
    "    logging.info(\"Printing sequences\")\n",
    "    for seq in sequences:\n",
    "        logging.info(f\"Result: {seq['generated_text']}\")\n",
    "    logging.info(\"Operation Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29a38c4f-03c8-4f55-862e-38722bf94613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 21:08:21,645 INFO 1843959056 <module> Asking Question...\n",
      "2023-09-01 21:08:26,617 INFO 1843959056 <module> Question Answered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The first man on the moon was the first person ever sent into space by NASA in 1954, but it was a Soviet spacecraft that launched the first human in space. Therefore, the first man to land on the moon was the cosmonaut Yuri Gagarin from the Soviet Union.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "    \n",
    "    template = \"\"\"Question: {question}\n",
    "    Answer: Let's think step by step.\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=template, \n",
    "        input_variables= [\"question\"]\n",
    "    )\n",
    "    \n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    logging.info(\"Asking Question...\")\n",
    "    output = llm_chain.run(\"Who was the first man on the moon?\")\n",
    "    print(output)\n",
    "    logging.info(\"Question Answered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9eea3-55db-40c7-81fe-e3426afe626f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
