{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a2d4a0-6bce-4114-81c7-9c8906d11d6a",
   "metadata": {},
   "source": [
    "# Day 0\n",
    "\n",
    "In this first notebook, our goal is to run the Falcon-7b-instruct model from [HuggingFace](https://huggingface.co/models) locally by applying quantization to the model\n",
    "and run inference on it with a prompt template from Langchain.\n",
    "\n",
    "First, let's verify that our GPU drivers are working correctly with the following checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bd1c6-d5ce-4d8c-961a-663da2c61f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679388e-2256-4d0c-b9b4-ced182fe7e10",
   "metadata": {},
   "source": [
    "Output of above cell should return correctly, and display something similar to below\n",
    "\n",
    "```bash\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0  On |                  N/A |\n",
    "|  0%   56C    P5              35W / 370W |    841MiB / 10240MiB |     69%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39ad89-d7bf-4e36-8d35-2f5db8fb9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(levelname)s %(module)s %(funcName)s %(message)s',\n",
    "    handlers=[\n",
    "        stream_handler,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c8a6c3-d511-4dbb-b918-26914ef2b66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(f\"PyTorch GPU Available: {torch.cuda.is_available()} GPU Count: {torch.cuda.device_count()} GPU Current: {torch.cuda.current_device()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    logging.info(f\"GPU Found: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a35124-479a-4daf-987d-bc8ea9028f7e",
   "metadata": {},
   "source": [
    "We can run the following cells to ensure that Accelerate is picking up our GPU and system resources correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c8a87-4165-4ebe-87db-b5ec5a0ff74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f55f5-ef14-4204-beb8-ded99d27fb1e",
   "metadata": {},
   "source": [
    "We can also run the `accelerate test` command to run a test suite for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32025321-6b9e-401a-a644-2860995c1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f6761-a138-440b-bc0d-534caf501223",
   "metadata": {},
   "source": [
    "# Brief Introduction to Quantization\n",
    "\n",
    "Quantization is the technique to reduce the memory size and computational costs of large models by reducing the numerical representations of the weights and activations from higher precision data types like float32, to lower precision data types like int8.This presents us the following benefits\n",
    "\n",
    "1. Lower requirement of RAM (memory) required to serve large models.\n",
    "2. Lower computational requirements; faster inference\n",
    "3. As a result of 1. and 2., lower monetary costs associated with training and performing inference.\n",
    "\n",
    "For a deeper dive into the topic, we can review the following readings:\n",
    "1. [HF-BitsAndBytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n",
    "2. [4bit-Transformers-BitsAndBytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
    "\n",
    "Let us prepare our quantization config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bcd22-a9dd-4d55-b134-b8c58a775615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, GenerationConfig, pipeline as tf_pipeline\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd424c46-e2e7-47db-9b45-c65fd10026ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    # Load pre-trained model in 4bit-quantization\n",
    "    load_in_4bit=True,\n",
    "    # Change PyTorch Computation type from float16 to bfloat16\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16,  # works for only GPU\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    # Normalized float 4 (4bit)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Perform a second quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345afaf6-4b40-4d8a-a06b-92a290019c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our desired HuggingFace Falcon Model\n",
    "MODEL_ID = \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affa6ef-e01c-4a24-bc05-e0ba820a3504",
   "metadata": {},
   "source": [
    "We can use the `accelerate estimate-memory` command to estimate our LLM model size and also the potential gains from quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d6105-3d10-43ae-b02b-b01163afb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate estimate-memory tiiuae/falcon-7b-instruct  --trust_remote_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cdfec-1128-4436-9590-812b98da3487",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "Next, we want to load the tokenizer for our model. The tokenizer converts our query, sentences into tokens that the LLM can run predictions on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ed849-23f7-45e1-9ce1-f089160599e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running Tokenizer ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a8897-13c5-485f-a323-309d71b86e1a",
   "metadata": {},
   "source": [
    "**Important Notes**\n",
    "After we installed the library accelerate, accelerate will automatically load the model into GPU memory if it can find a GPU device.\n",
    "\n",
    "Be careful not to run the below cell twice, as this will load the model twice into the GPU\n",
    "\n",
    "We can run `!nvidia-smi` cell to check the GPU memory usage after loading the model.\n",
    "\n",
    "For our Falcon-7b instruct model, the on-disk size is 14gb+, after quantization, we only use *4.5gb*! Shrinking the model size by **3x**!\n",
    "```bash\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0  On |                  N/A |\n",
    "|  0%   59C    P2             106W / 370W |   5493MiB / 10240MiB |      1%      Default\n",
    "```\n",
    "\n",
    "In the event, we want to free up the GPU memory, we can `Restart Kernel and Clear Outputs of all Cells`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49ca65-a23e-4754-addc-c2350a9ddc6e",
   "metadata": {},
   "source": [
    "# AutoModels\n",
    "We can use HuggingFace's **AutoModelForCasualLM** to quickly download the base model `falcon-7b-instruct` and apply quantization to it. The model gets saved to `$HOME/.cache/huggingface/hub` and 7b-instruct needs about 15gb of storage to be conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797eb9ed-33a7-47d8-ba19-2bbfaee0d1d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"Loading Model ...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd9dfd0-224c-48f5-be27-dca7b0e7b71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "model.config.model_type = \"falcon\"\n",
    "model_better = BetterTransformer.transform(model, keep_original_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7456cdb-dadb-4058-a5ab-b1c3618dfc8a",
   "metadata": {},
   "source": [
    "After we have downloaded our model, we can use it in a pipeline to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f7038-a699-487e-b7ad-f80601223e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Creating text generation pipeline ...\")\n",
    "\n",
    "pipeline = tf_pipeline(\n",
    "    \"text-generation\",\n",
    "    #model=model,\n",
    "    model=model_better,\n",
    "    use_cache=True,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    max_length=500,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0fafc-97bd-43e2-b1a5-4c1b6073fd25",
   "metadata": {},
   "source": [
    "Let us create a test scenario of a School Parent Meeting (SPM) between a teacher Sally, a student Val and her mother Soph, and ask Falcon to generate a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45f10a-bacd-4a50-88d5-b3592f9449ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generating sequences\")\n",
    "sequences = pipeline(\n",
    "   \"\"\"Sally is a kindergarden teacher and Val is her student.Sally needs to contact Val's parent Soph to discuss her child's performance in school.Val is always late for class, and Sally needs to highlight to Soph about it.Val is extroverted and has made many new friends in school.\n",
    "   Soph: Morning Sally! How is Val doing in school?\n",
    "   Sally:\"\"\",\n",
    ")\n",
    "logging.info(\"Printing sequences\")\n",
    "for seq in sequences:\n",
    "    logging.info(f\"Result: {seq['generated_text']}\")\n",
    "logging.info(\"Operation Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819d28f-fb18-45c3-ab5a-5bd42a9dc725",
   "metadata": {},
   "source": [
    "# Integration with Langchain\n",
    "Once we have our huggingface pipeline made, it is easy to integrate it with Langchain for an easier interface to prompting.A warning, vanilla Falcon-7b tends to hallucinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76efab-a1c0-455b-9d2f-2b7cb9f3438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "template = \"\"\"Scenario: You (Sally) are a math kindergarden teacher who is meeting parents and students for their bi-annual semester assessment. You\n",
    "should provide feedback based on each student's grade this semester.Grades are defined in descending order of: A, B, C ,D ,E , F. D, E and F are failing grades.\n",
    "\n",
    "Example Question Format:\n",
    "Grade:\n",
    "Parent Name:\n",
    "Sally:\n",
    "\n",
    "Example:\n",
    "Grade: C\n",
    "Claire: Hi Sally, I am Sarah's parent. Has her math improved this semester?\n",
    "Sally: Hi Claire, she scored a C on her math exam, and has room for improvement, I would suggest more revision and exercises for her.\n",
    "\n",
    "Example:\n",
    "Grade: A\n",
    "Sam: Hi Sally, I am Tom's parent. How is Tom doing in exams?\n",
    "Sally: Tom demonstrates keen interest in the subject, scoring a remarkable A, keep up the good work.\n",
    "\n",
    "\n",
    "Be concise.Generate one response only.Answer the parent's question below.\n",
    "\n",
    "Grade: {grade}\n",
    "{parent}: Hi Sally, I am  {student}'s parent. {question}\n",
    "Sally:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables= [\"grade\", \"parent\", \"student\", \"question\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a57a-5f62-40df-8981-6bb1bb68a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Asking Question...\")\n",
    "output = llm_chain.run({\"grade\": \"A\", \"parent\": \"Peter\", \"student\": \"Becky\", \"question\": \"How did Becky perform this semester?\"})\n",
    "logging.info(output)\n",
    "logging.info(\"Question Answered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8490e8-74dd-44e0-be26-f0a0cc5efb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
