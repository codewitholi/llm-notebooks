{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a2d4a0-6bce-4114-81c7-9c8906d11d6a",
   "metadata": {},
   "source": [
    "# Day 0\n",
    "\n",
    "In this first notebook, our goal is to run the Falcon-7b-instruct model from [HuggingFace](https://huggingface.co/models) locally by applying quantization to the model\n",
    "and run inference on it with a prompt template from Langchain.\n",
    "\n",
    "First, let's verify that our GPU drivers are working correctly with the following checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97bd1c6-d5ce-4d8c-961a-663da2c61f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 23 05:17:17 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   48C    P3              63W / 370W |   1494MiB / 10240MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2058      G   /usr/lib/xorg/Xorg                          747MiB |\n",
      "|    0   N/A  N/A      2179      G   /usr/bin/gnome-shell                        493MiB |\n",
      "|    0   N/A  N/A      2592      G   /usr/lib/firefox-esr/firefox-esr            232MiB |\n",
      "|    0   N/A  N/A      3930      G   gnome-control-center                          3MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c679388e-2256-4d0c-b9b4-ced182fe7e10",
   "metadata": {},
   "source": [
    "Output of above cell should return correctly, and display something similar to below\n",
    "\n",
    "```bash\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0  On |                  N/A |\n",
    "|  0%   56C    P5              35W / 370W |    841MiB / 10240MiB |     69%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a39ad89-d7bf-4e36-8d35-2f5db8fb9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s %(levelname)s %(module)s %(funcName)s %(message)s',\n",
    "    handlers=[\n",
    "        stream_handler,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c8a6c3-d511-4dbb-b918-26914ef2b66a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:18,168 INFO 2625440443 <module> PyTorch GPU Available: True GPU Count: 1 GPU Current: 0\n",
      "2023-09-23 05:17:18,169 INFO 2625440443 <module> GPU Found: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"PyTorch GPU Available: {torch.cuda.is_available()} GPU Count: {torch.cuda.device_count()} GPU Current: {torch.cuda.current_device()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    logging.info(f\"GPU Found: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a35124-479a-4daf-987d-bc8ea9028f7e",
   "metadata": {},
   "source": [
    "We can run the following cells to ensure that Accelerate is picking up our GPU and system resources correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8c8a87-4165-4ebe-87db-b5ec5a0ff74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copy-and-paste the text below in your GitHub issue\n",
      "\n",
      "- `Accelerate` version: 0.23.0\n",
      "- Platform: Linux-5.10.0-25-amd64-x86_64-with-glibc2.31\n",
      "- Python version: 3.10.11\n",
      "- Numpy version: 1.26.0\n",
      "- PyTorch version (GPU?): 2.0.1+cu117 (True)\n",
      "- PyTorch XPU available: False\n",
      "- PyTorch NPU available: False\n",
      "- System RAM: 62.01 GB\n",
      "- GPU type: NVIDIA GeForce RTX 3080\n",
      "- `Accelerate` default config:\n",
      "\tNot found\n"
     ]
    }
   ],
   "source": [
    "!accelerate env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f55f5-ef14-4204-beb8-ded99d27fb1e",
   "metadata": {},
   "source": [
    "We can also run the `accelerate test` command to run a test suite for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32025321-6b9e-401a-a644-2860995c1e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running:  accelerate-launch /home/kirito/repos/llm-notebooks/.venv/lib/python3.10/site-packages/accelerate/test_utils/scripts/test_script.py\n",
      "stderr: The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "stderr: \t`--num_processes` was set to a value of `1`\n",
      "stderr: \t`--num_machines` was set to a value of `1`\n",
      "stderr: \t`--mixed_precision` was set to a value of `'no'`\n",
      "stderr: \t`--dynamo_backend` was set to a value of `'no'`\n",
      "stderr: To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "stdout: **Initialization**\n",
      "stdout: Testing, testing. 1, 2, 3.\n",
      "stdout: Distributed environment: NO\n",
      "stdout: Num processes: 1\n",
      "stdout: Process index: 0\n",
      "stdout: Local process index: 0\n",
      "stdout: Device: cuda\n",
      "stdout: \n",
      "stdout: Mixed precision type: no\n",
      "stdout: \n",
      "stdout: \n",
      "stdout: **Test process execution**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a list**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a dict**\n",
      "stdout: \n",
      "stdout: **Test split between processes as a tensor**\n",
      "stdout: \n",
      "stdout: **Test random number generator synchronization**\n",
      "stdout: All rng are properly synched.\n",
      "stdout: \n",
      "stdout: **DataLoader integration test**\n",
      "stdout: 0 tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "stdout:         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],\n",
      "stdout:        device='cuda:0') <class 'accelerate.data_loader.DataLoaderShard'>\n",
      "stdout: Non-shuffled dataloader passing.\n",
      "stdout: Shuffled dataloader passing.\n",
      "stdout: Non-shuffled central dataloader passing.\n",
      "stdout: Shuffled central dataloader passing.\n",
      "stdout: \n",
      "stdout: **Training integration test**\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributed setup with no batch split.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Training yielded the same results on one CPU or distributes setup with batch split.\n",
      "stdout: FP16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: Keep fp32 wrapper check.\n",
      "stdout: BF16 training check.\n",
      "stdout: Model dtype: torch.float32, torch.float32. Input dtype: torch.float32\n",
      "stdout: \n",
      "stdout: **Breakpoint trigger test**\n",
      "Test is a success! You are ready for your distributed training!\n"
     ]
    }
   ],
   "source": [
    "!accelerate test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72f6761-a138-440b-bc0d-534caf501223",
   "metadata": {},
   "source": [
    "# Brief Introduction to Quantization\n",
    "\n",
    "Quantization is the technique to reduce the memory size and computational costs of large models by reducing the numerical representations of the weights and activations from higher precision data types like float32, to lower precision data types like int8.This presents us the following benefits\n",
    "\n",
    "1. Lower requirement of RAM (memory) required to serve large models.\n",
    "2. Lower computational requirements; faster inference\n",
    "3. As a result of 1. and 2., lower monetary costs associated with training and performing inference.\n",
    "\n",
    "For a deeper dive into the topic, we can review the following readings:\n",
    "1. [HF-BitsAndBytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n",
    "2. [4bit-Transformers-BitsAndBytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
    "\n",
    "Let us prepare our quantization config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e33bcd22-a9dd-4d55-b134-b8c58a775615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:23,245 INFO instantiator <module> Created a temporary directory at /tmp/tmp1h77a0cm\n",
      "2023-09-23 05:17:23,246 INFO instantiator _write Writing /tmp/tmp1h77a0cm/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, GenerationConfig, pipeline as tf_pipeline\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd424c46-e2e7-47db-9b45-c65fd10026ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    # Load pre-trained model in 4bit-quantization\n",
    "    load_in_4bit=True,\n",
    "    # Change PyTorch Computation type from float16 to bfloat16\n",
    "    # bnb_4bit_compute_dtype=torch.bfloat16,  # works for only GPU\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    # Normalized float 4 (4bit)\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # Perform a second quantization\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345afaf6-4b40-4d8a-a06b-92a290019c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify our desired HuggingFace Falcon Model\n",
    "MODEL_ID = \"tiiuae/falcon-7b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2affa6ef-e01c-4a24-bc05-e0ba820a3504",
   "metadata": {},
   "source": [
    "We can use the `accelerate estimate-memory` command to estimate our LLM model size and also the potential gains from quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b89d6105-3d10-43ae-b02b-b01163afb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained config for `tiiuae/falcon-7b-instruct` from `transformers`...\n",
      "┌────────────────────────────────────────────────────────┐\n",
      "│  Memory Usage for loading `tiiuae/falcon-7b-instruct`  │\n",
      "├───────┬─────────────┬──────────┬───────────────────────┤\n",
      "│ dtype │Largest Layer│Total Size│  Training using Adam  │\n",
      "├───────┼─────────────┼──────────┼───────────────────────┤\n",
      "│float32│    1.1 GB   │ 26.89 GB │       107.54 GB       │\n",
      "│float16│  563.56 MB  │ 13.44 GB │        53.77 GB       │\n",
      "│  int8 │  281.78 MB  │ 6.72 GB  │        26.89 GB       │\n",
      "│  int4 │  140.89 MB  │ 3.36 GB  │        13.44 GB       │\n",
      "└───────┴─────────────┴──────────┴───────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "!accelerate estimate-memory tiiuae/falcon-7b-instruct  --trust_remote_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cdfec-1128-4436-9590-812b98da3487",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "Next, we want to load the tokenizer for our model. The tokenizer converts our query, sentences into tokens that the LLM can run predictions on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da3ed849-23f7-45e1-9ce1-f089160599e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:26,001 INFO 3973576235 <module> Running Tokenizer ...\n",
      "2023-09-23 05:17:26,005 DEBUG connectionpool _new_conn Starting new HTTPS connection (1): huggingface.co:443\n",
      "2023-09-23 05:17:26,279 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Running Tokenizer ...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a8897-13c5-485f-a323-309d71b86e1a",
   "metadata": {},
   "source": [
    "**Important Notes**\n",
    "After we installed the library accelerate, accelerate will automatically load the model into GPU memory if it can find a GPU device.\n",
    "\n",
    "Be careful not to run the below cell twice, as this will load the model twice into the GPU\n",
    "\n",
    "We can run `!nvidia-smi` cell to check the GPU memory usage after loading the model.\n",
    "\n",
    "For our Falcon-7b instruct model, the on-disk size is 14gb+, after quantization, we only use *4.5gb*! Shrinking the model size by **3x**!\n",
    "```bash\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3080        On  | 00000000:01:00.0  On |                  N/A |\n",
    "|  0%   59C    P2             106W / 370W |   5493MiB / 10240MiB |      1%      Default\n",
    "```\n",
    "\n",
    "In the event, we want to free up the GPU memory, we can `Restart Kernel and Clear Outputs of all Cells`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49ca65-a23e-4754-addc-c2350a9ddc6e",
   "metadata": {},
   "source": [
    "# AutoModels\n",
    "We can use HuggingFace's **AutoModelForCasualLM** to quickly download the base model `falcon-7b-instruct` and apply quantization to it. The model gets saved to `$HOME/.cache/huggingface/hub` and 7b-instruct needs about 15gb of storage to be conservative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "797eb9ed-33a7-47d8-ba19-2bbfaee0d1d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:26,321 INFO 1754090134 <module> Loading Model ...\n",
      "2023-09-23 05:17:26,579 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-09-23 05:17:26,831 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/configuration_RW.py HTTP/1.1\" 200 0\n",
      "2023-09-23 05:17:27,096 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2023-09-23 05:17:27,390 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/modelling_RW.py HTTP/1.1\" 200 0\n",
      "2023-09-23 05:17:28,357 INFO modeling get_balanced_memory We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4102eecc16f4d4b9557adec192bb5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:32,458 DEBUG connectionpool _make_request https://huggingface.co:443 \"HEAD /tiiuae/falcon-7b-instruct/resolve/main/generation_config.json HTTP/1.1\" 200 0\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Loading Model ...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7456cdb-dadb-4058-a5ab-b1c3618dfc8a",
   "metadata": {},
   "source": [
    "After we have downloaded our model, we can use it in a pipeline to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "942f7038-a699-487e-b7ad-f80601223e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:32,481 INFO 2212159532 <module> Creating text generation pipeline ...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Creating text generation pipeline ...\")\n",
    "\n",
    "pipeline = tf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    use_cache=True,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    max_length=500,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0fafc-97bd-43e2-b1a5-4c1b6073fd25",
   "metadata": {},
   "source": [
    "Let us create a test scenario of a School Parent Meeting (SPM) between a teacher Sally, a student Val and her mother Soph, and ask Falcon to generate a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c45f10a-bacd-4a50-88d5-b3592f9449ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:17:32,486 INFO 2580241112 <module> Generating sequences\n",
      "/home/kirito/repos/llm-notebooks/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "2023-09-23 05:17:39,647 INFO 2580241112 <module> Printing sequences\n",
      "2023-09-23 05:17:39,648 INFO 2580241112 <module> Result: Sally is a kindergarden teacher and Val is her student.Sally needs to contact Val's parent Soph to discuss her child's performance in school.Val is always late for class, and Sally needs to highlight to Soph about it.Val is extroverted and has made many new friends in school.\n",
      "   Soph: Morning Sally! How is Val doing in school?\n",
      "   Sally: Hi, Soph! Val is enjoying school, but she's late often. I think it might be affecting her learning.\n",
      "    Soph: Hi Val, how's school going? Val is really enjoying it, and making a lot of new friends. But she's been late quite a lot. We think it's affecting her learning.\n",
      "2023-09-23 05:17:39,648 INFO 2580241112 <module> Operation Complete\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Generating sequences\")\n",
    "sequences = pipeline(\n",
    "   \"\"\"Sally is a kindergarden teacher and Val is her student.Sally needs to contact Val's parent Soph to discuss her child's performance in school.Val is always late for class, and Sally needs to highlight to Soph about it.Val is extroverted and has made many new friends in school.\n",
    "   Soph: Morning Sally! How is Val doing in school?\n",
    "   Sally:\"\"\",\n",
    ")\n",
    "logging.info(\"Printing sequences\")\n",
    "for seq in sequences:\n",
    "    logging.info(f\"Result: {seq['generated_text']}\")\n",
    "logging.info(\"Operation Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0819d28f-fb18-45c3-ab5a-5bd42a9dc725",
   "metadata": {},
   "source": [
    "# Integration with Langchain\n",
    "Once we have our huggingface pipeline made, it is easy to integrate it with Langchain for an easier interface to prompting.A warning, vanilla Falcon-7b tends to hallucinate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d76efab-a1c0-455b-9d2f-2b7cb9f3438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-23 05:47:49,384 INFO 2510736692 <module> Asking Question...\n",
      "2023-09-23 05:48:02,626 INFO 2510736692 <module> Question Answered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be concise. Generate one response only. Answer the parent's question below.\n",
      "\n",
      "Grade: B\n",
      "Alice: Hi Sally, I am John's parent. How is he doing academically this semester? Please provide a feedback.\n",
      "Sally: John's overall performance is a bit mixed, although he's made substantial progress in the past 2 months. His current math level is B, but he needs more consistent practice to reach a higher level.\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline)\n",
    "\n",
    "template = \"\"\"Scenario: You (Sally) are a math kindergarden teacher who is meeting parents and students for their bi-annual semester assessment. You\n",
    "should provide feedback based on each student's performance this semester. Be concise.\n",
    "\n",
    "Grades are defined in descending order of: A, B, C ,D ,E , F. D, E and F are failing grades. Generate one response only.\n",
    "\n",
    "Example:\n",
    "Grade: C\n",
    "Claire: Hi Sally, I am Sarah's parent. Has her math improved this semester?\n",
    "Sally: Hi Claire, she scored a C on her math exam, and has room for improvement, I would suggest more revision and exercises for her.\n",
    "\n",
    "Example:\n",
    "Grade: A\n",
    "Sam: Hi Sally, I am Tom's parent. How is Tom doing in exams?\n",
    "Sally: Tom demonstrates keen interest in the subject, scoring a remarkable A, keep up the good work.\n",
    "\n",
    "\n",
    "Be concise.Generate one response only.Answer the parent's question below.\n",
    "\n",
    "Grade: {grade}\n",
    "{parent}: Hi Sally, I am  {student}'s parent. {question}\n",
    "Sally:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables= [\"grade\", \"parent\", \"student\", \"question\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "logging.info(\"Asking Question...\")\n",
    "output = llm_chain.run({\"grade\": \"A\", \"parent\": \"Peter\", \"student\": \"Becky\", \"question\": \"How did Becky perform this semester?\"})\n",
    "logger.info(output)\n",
    "logging.info(\"Question Answered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a57a-5f62-40df-8981-6bb1bb68a733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
